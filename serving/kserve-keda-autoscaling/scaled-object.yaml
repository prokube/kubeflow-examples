# KEDA ScaledObject for KServe InferenceService with vLLM backend
# Scales based on custom Prometheus metrics from vLLM serving runtime
#
# Prerequisites:
# - KEDA installed in cluster (https://keda.sh/docs/deploy/)
# - Prometheus collecting vLLM metrics (see service-monitor.yaml)
#
# Note: vLLM uses colons in metric names (e.g., vllm:num_requests_running),
# which is unusual but correct. Use {"__name__"="..."} syntax in PromQL.
#
# TODO: Replace <your-namespace> with your actual namespace in the queries below
#
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: opt-125m-vllm-scaledobject
  labels:
    app: opt-125m-vllm
spec:
  # Target the KServe predictor deployment
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: opt-125m-vllm-predictor-00001-deployment
  # Polling interval for checking metrics (seconds)
  pollingInterval: 15
  # Cooldown period before scaling down (seconds)
  cooldownPeriod: 60
  # Min/max replicas
  minReplicaCount: 1
  maxReplicaCount: 10
  # Advanced scaling behavior
  advanced:
    horizontalPodAutoscalerConfig:
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 120
          policies:
            - type: Percent
              value: 25
              periodSeconds: 60
        scaleUp:
          stabilizationWindowSeconds: 0
          policies:
            - type: Percent
              value: 100
              periodSeconds: 15
            - type: Pods
              value: 4
              periodSeconds: 15
          selectPolicy: Max
  triggers:
    # Scale based on Time To First Token (TTFT) - P95
    # Scale up when P95 TTFT exceeds 200ms (0.2s)
    - type: prometheus
      metadata:
        serverAddress: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090/prometheus
        metricName: vllm_ttft_p95
        query: |
          histogram_quantile(0.95, sum(rate({"__name__"="vllm:time_to_first_token_seconds_bucket", namespace="<your-namespace>"}[2m])) by (le))
        threshold: "0.2"
        activationThreshold: "0.1"
    # Scale based on GPU KV-cache usage (for GPU deployments)
    # Scale up when cache usage exceeds 70%
    - type: prometheus
      metadata:
        serverAddress: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090/prometheus
        metricName: vllm_gpu_cache_usage
        query: |
          avg({"__name__"="vllm:gpu_cache_usage_perc", namespace="<your-namespace>"})
        threshold: "0.7"
        activationThreshold: "0.5"
    # Fallback: Scale based on running requests (always works)
    - type: prometheus
      metadata:
        serverAddress: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090/prometheus
        metricName: vllm_num_requests_running
        query: |
          avg({"__name__"="vllm:num_requests_running", namespace="<your-namespace>"})
        threshold: "2"
        activationThreshold: "1"
