# KEDA ScaledObject for KServe InferenceService with vLLM backend
# Scales based on custom Prometheus metrics from vLLM serving runtime
#
# Prerequisites:
# - KEDA installed in cluster (https://keda.sh/docs/deploy/)
# - Prometheus collecting vLLM metrics (see service-monitor.yaml)
#
# Note: vLLM metrics use colons in names (e.g., vllm:num_requests_running)
# which need to be quoted in PromQL queries
#
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: opt-125m-vllm-scaledobject
  labels:
    app: opt-125m-vllm
spec:
  # Target the KServe predictor deployment
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: opt-125m-vllm-predictor-00001-deployment
  # Polling interval for checking metrics (seconds)
  pollingInterval: 15
  # Cooldown period before scaling down (seconds)  
  cooldownPeriod: 60
  # Min/max replicas
  minReplicaCount: 1
  maxReplicaCount: 10
  # Advanced scaling behavior
  advanced:
    horizontalPodAutoscalerConfig:
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 120
          policies:
            - type: Percent
              value: 25
              periodSeconds: 60
        scaleUp:
          stabilizationWindowSeconds: 0
          policies:
            - type: Percent
              value: 100
              periodSeconds: 15
            - type: Pods
              value: 4
              periodSeconds: 15
          selectPolicy: Max
  triggers:
    # Scale based on number of running requests per pod
    # vLLM uses colons in metric names, so we use the actual metric name
    - type: prometheus
      metadata:
        serverAddress: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090/prometheus
        metricName: vllm_num_requests_running
        # Scale up when average running requests per pod > 2
        query: |
          avg({"__name__"="vllm:num_requests_running", namespace="developer1"})
        threshold: "2"
        activationThreshold: "1"
    # Scale based on number of waiting requests (queue depth)
    - type: prometheus
      metadata:
        serverAddress: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090/prometheus
        metricName: vllm_num_requests_waiting
        # Scale up when there are waiting requests
        query: |
          sum({"__name__"="vllm:num_requests_waiting", namespace="developer1"})
        threshold: "1"
        activationThreshold: "0"
